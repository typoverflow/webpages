<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description"
    content="Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <link rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.1.0/prism-bibtex.min.js"></script>
  <link rel="stylesheet" href="static/css/index.css" />

  <title>Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning</title>

  <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  <script>
    function selectContent(query) {
      var range = document.createRange()
      var selection = window.getSelection()
      var elem = document.querySelector(query)
      range.selectNodeContents(elem)
      selection.removeAllRanges()
      selection.addRange(range)
    }
  </script>
</head>

<body>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
        displayMath: [
          ["$$", "$$"],
          ["\\[", "\\]"],
        ],
        processEscapes: true,
        macros: {
          planclass: "\\operatorname{\\mathbf{plan}}",
          S: "\\mathcal{S}",
          gS: "\\mathcal{S}",
          cS: "\\mathcal{S}",
          cB: "\\mathcal{B}",
          cA: "\\mathcal{A}",
          A: "\\mathcal{A}",
          gA: "\\mathcal{A}",
          D: "\\mathcal{D}",
          R: "\\mathbb{R}",
          E: "\\mathbb{E}",
          P: "\\mathrm{P}",
          var: "\\mathrm{Var}",
          cov: "\\mathrm{Cov}",
          argmin: "\\mathop{\\arg\\min}",
          argmax: "\\mathop{\\arg\\max}",
        },
      },
      svg: {
        fontCache: "global",
      },
    }
  </script>
  <header>
    <h1>Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning</h1>
    <div class="authors">
      <span class="author">
        <a href="https://about.gaocx.io">Chen-Xiao Gao</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Chenyang Wu</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Mingjun Cao</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Chenjun Xiao</a></span>
      <span class="affil">2</span>
      <span class="author">
        <a>Yang Yu</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Zongzhang Zhang</a></span>
      <span class="affil">1</span>
    </div>
    <div class="notes">
      <span class="affil">1</span>
      <span class="institution">Nanjing University</span>
      <span class="affil">2</span>
      <span class="institution">The Chinese University of Hong Kong, Shenzhen</span>
    </div>
    <div class="links">
      <span class="link">
        <a href="./static/pdf/bdpo.pdf" target="_blank" class="button">
          <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
        </a>
      </span>
      <span class="link">
        <a href="https://arxiv.org/abs/2502.04778" class="button">
          <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
        </a>
      </span>
      <span class="link">
        <a href="https://github.com/typoverflow/flow-rl" class="button">
          <span class="icon"><i class="fas fa-code"></i></span><span>Code</span>
        </a>
      </span>
    </div>
  </header>

  <main>
    <section>
      <div class="summary">
        <h3 style="text-align: center; color: #27579a; margin-bottom: 0px">TL;DR</h3>
        <div class="yspace centered">
          <div class="full-width margin" 
               style="width: 100%; margin: 0; display: flex; align-items: center; justify-content: center; gap: 20px;">
            <div style="width: 45%; margin: 0; text-align: center;">
              <img src="static/figures/svg/path.svg" width="100%" />
            </div>
            <p class="caption" style="width: 45%; margin: 0;">
              Unimodal policies, such as deterministic policies, compute the behavior as the center of mass and therefore lead to misleading regularization; while we harnesses the flexibility of diffusion models, and the regularization is calculated as the accumulated discrepancies in diffusion directions of the actor and the behavior diffusion. This motivates BDPO, our efficient algorithm for optimizing diffusion policies. 
            </p>
          </div>
        </div>
      </div>
    </section>

    <section>
      <div class="abstract">
        <h3>Abstract</h3>
        <p>
          Behavior regularization, which constrains the policy to stay close to some behavior policy, is widely used in offline reinforcement learning to manage the risk of hazardous exploitation of unseen actions.
          Nevertheless, existing literature on behavior-regularized RL primarily focuses on explicit policy parameterizations, such as Gaussian policies. 
          Consequently, it remains unclear how to extend this framework to more advanced policy parameterizations, such as diffusion models. 
          In this paper, we introduce BDPO, a principled behavior-regularized RL framework tailored for diffusion-based policies, thereby combining the expressive power of diffusion policies and the robustness provided by regularization. 
          The key ingredient of our method is to calculate the Kullback-Leibler (KL) regularization analytically as the accumulated discrepancies in reverse-time transition kernels along the diffusion trajectory. 
          By integrating the regularization, we develop an efficient two-time-scale actor-critic RL algorithm that produces the optimal policy while respecting the behavior constraint. 
          Comprehensive evaluations conducted on synthetic 2D tasks and continuous control tasks from the D4RL benchmark validate its effectiveness and superior performance. 
        </p>
      </div>
    </section>

    <section>
      <h2>üßó Difficulties of Combining Diffusion Policies and Offline RL</h2>
          <p>
          The popularized behavior-regularized RL framework designs to optimize policies $\pi$ while also constraining them towards a behavior policy $\nu$: 
          $$\max_\pi\ \mathbb{E}_{a\sim\pi(\cdot|s)}\left[Q(s, a)\right]-\eta D_\mathrm{KL}(\pi\|\nu).$$
          By specifying $\nu$ as ...
          <ul style="list-style-type: disc; margin-left: 20px;">
            <li>
              <b>the uniform policy $U[\mathcal{A}]$:</b> it encourages diversity and exploration of the policy, recovering Maximum Entropy RL; 
            </li>
            <li>
              <b>the dataset collection policy $\pi_{\mathcal{D}}$:</b> it restricts the policy to center around the dataset $\mathcal{D}$, balancing optimization and safety;
            </li>
            <li>
              <b>the pre-trained or sft policy $\pi_{\mathrm{ref}}$:</b> it prevent the mode collapse during post-training. 
            </li>
          </ul>
          However, combining diffusion policies with the behavior-regularized RL framework is non-trivial.  
          </p>
          <div class="columns bypad centered" style="width: 100%; margin: 0; display: flex;">
            <div style="color: #c61010; width: 48%; margin-right: 2%;">
              <p class="caption">
                <b>‚ùå Problem 1: Regularization Calculation</b><br />
                Traditional regularization, such as KL divergence, requires action densities, which is not directly available for diffusion policies. 
              </p>
            </div>
            <div style="color: #c61010; width: 48%; margin-left: 2%; margin-top: -0px;">
              <p class="caption">
                <b>‚ùå Problem 2: Diffusion Policy Optimization</b><br />
                Traditional training objectives of diffusion models require samples from the target distribution, while in offline RL, we only have access to samples from the behavior policy $\nu$.
              </p>
            </div>
          </div>
    </section>

    <section>
      <h2>üß© The Ingredients of BDPO</h2>
      <div class="building-blocks-grid">

        <div class="block full-width" style="margin-top: 0;">
          <div style="display: flex; align-items: center; gap: 20px;">
            <div style="flex: 1; margin: 0;">
              <h3 style="margin-top: 0;">1. Pathwise KL Regularization</h3>
              <p style="margin: 0;">
                Instead of the considering the KL divergence between action distributions, we consider the KL divergence defined on diffusion paths, which can be further decomposed into the accumulated discrepancies in reverse diffusion directions:
                $$
                \begin{aligned}
                &D_{\rm KL}(p^{\pi, s}_{0:N} \| p^{\nu, s}_{0:N})\\
                &=\mathbb{E}\left[\log \frac{p_N^{\pi,s}(a^N)\prod_{n=1}^Np_{n-1|n}^{\pi,s}(a^{n-1}|a^n)}{p_N^{\nu,s}(a^N)\prod_{n=1}^Np_{n-1|n}^{\nu,s}(a^{n-1}|a^n)}\right]\\
                &=\mathbb{E}\left[\sum_{n=1}^{N}D_{\rm KL}(p^{\pi, s,a^n}_{n-1|n}\|p^{\nu, s,a^n}_{n-1|n})\right]\\
                &=\mathbb{E}\left[\sum_{n=1}^{N}\frac{\|\mu^{\pi,s}_n(a^n)-\mu^{\nu,s}_n(a^n)\|^2}{2\sigma_n^2}\right].
                \end{aligned}
                $$
                We therefore consider using the pathwise KL regularized objective:
                $$
                \max_{p^\pi}\ \mathbb{E}_{a^0\sim p^{\pi,s}}\left[Q(s, a^0)\right]-\eta D_{\rm KL}(p^{\pi, s}_{0:N} \| p^{\nu, s}_{0:N}).
                $$
              </p>
              <p style="color: #27579a">
                ‚úî We proved the equivalency between pathwise KL regularization and KL regularization. <br />
                ‚úî The pathwise KL is consistent with the KL between two diffusion SDEs in the limit of infinitesimal step sizes.
              </p>
            </div>
          </div>
        </div>

        <div class="block full-width" style="margin-top: 0;">
          <div style="display: flex; align-items: center; gap: 20px;">
            <div style="flex: 1; margin: 0;">
              <h3 style="margin-top: 0;">2. Two-time-scale Actor-Critic</h3>
              <p style="margin: 0;">
                Now we are dealing with a two-time-scale RL algorithm. The upper level works in the environment time steps, which specifies the reward minus the penalty as single step reward; 
                the lower level works completely inside the diffusion time steps, which treats single step KL as the reward. 
                <br />
                Similar to how temporal difference (TD) learning amortizes the cost for trajectory optimization, we employ two-time-scale TD learning:
              </p>
              <p>
                1. (Upper Level): The action value functions $Q^\pi(s, a)$ are estimated by standard TD learning: <br />
                $$Q^\pi(s, a)\leftarrow R(s, a) + \gamma \mathbb{E}_{a'^{0:N}\sim p^{\pi,s}_{0:N}}\left[Q^\pi(s', a'^0)-\sum_{n=1}^ND_{\rm KL}(p^{\pi,s,a'^n}_{n-1|n}\|p^{\nu,s,a'^n}_{n-1|n})\right]$$
                2. (Lower Level): The diffusion value functions $V^{\pi, s}_n$ are estimated by TD learning between single step reverse diffusion:
                $$
                \begin{aligned}
                &V_0^{\pi,s}(a^0)\leftarrow Q^\pi(s,a^0),\\
                &V_n^{\pi,s}(a^n)\leftarrow-\eta D_{\rm KL}(p^{\pi,s,a^n}_{n-1|n}\|p^{\nu,s,a^n}_{n-1|n})+\mathbb{E}_{a^{n-1}\sim p^{\pi,s,a^n}_{n-1|n}}\left[V_{n-1}^{\pi,s}(a^{n-1})\right].
            \end{aligned}
                $$
                3. (Policy Improvement): The diffusion policy now only requires single step diffusion to improve!
                $$
                \begin{aligned}
                &\max_{p^{\pi,s,a^n}_{n-1|n}}\ \ -\eta\ell^{\pi,s}_n(a^n) + \underset{p^{\pi,s,a^n}_{n-1|n}}{\mathbb{E}}\left[V^{\pi,s}_{n-1}(a^{n-1})\right].
                \end{aligned}
                $$
              </p>
              <p style="color: #27579a">
                ‚úî We proved the convergence of the two-time-scale actor-critic algorithm. <br />
              </p>
            </div>
          </div>
        </div>
      </div>

      <style>
        .building-blocks-grid {
          display: grid;
          grid-template-columns: repeat(2, 1fr);
          gap: 20px;
          margin: 20px 0;
          perspective: 1000px;
        }

        .block {
          background-color: #f5f5f5;
          border: 1px solid #ddd;
          border-radius: 8px;
          padding: 20px;
          transition: all 0.3s ease;
          position: relative;
          overflow: hidden;
          transform-style: preserve-3d;
        }

        .block h3 {
          color: #27579a;
          margin-top: 0;
        }

        .block p {
          margin-bottom: 0;
        }

        .full-width {
          grid-column: 1 / -1;
          margin-top: 20px;
        }

        /* Add hover effects */
        .block:hover {
          transform: translateY(-5px);
          box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
        }

        h3#qualitative-evaluation {
          scroll-margin-top: 20px;
          text-align: center;
          color: #27579a;
        }
      </style>

      <script>
        document.addEventListener('DOMContentLoaded', () => {
          const blocks = document.querySelectorAll('.block');

          blocks.forEach(block => {
            block.addEventListener('mousemove', (e) => {
              const rect = block.getBoundingClientRect();
              const x = e.clientX - rect.left;
              const y = e.clientY - rect.top;

              const centerX = rect.width / 2;
              const centerY = rect.height / 2;

              // Special case for full-width block - reduce rotation by half
              const divisor = block.classList.contains('full-width') ? 1500 : 320;
              const rotateX = -(y - centerY) / divisor;
              const rotateY = (x - centerX) / divisor;

              block.style.transform = `
                                perspective(1000px)
                                rotateX(${rotateX}deg)
                                rotateY(${rotateY}deg)
                                scale3d(1.005, 1.005, 1.005)
                            `;

              block.style.boxShadow = `
                                ${-rotateY / 16}px ${rotateX / 16}px 20px rgba(39, 87, 207, 0.15),
                                ${-rotateY / 32}px ${rotateX / 32}px 10px rgba(39, 87, 207, 0.11)
                            `;
            });

            block.addEventListener('mouseleave', () => {
              block.style.transform = 'perspective(1000px) rotateX(0) rotateY(0) scale3d(1, 1, 1)';
              block.style.boxShadow = 'none';
            });
          });
        });
      </script>
    </section>

    <section>
      <h2>üîç Key Empirical Findings</h2>

      <h3 style="text-align: center; color: #27579a; margin-bottom: 0px" id="toy2d">Synthetic 2D Tasks
      </h3>
      <div class="yspace centered">
        <div class="full-width margin"
          style="width: 100%; margin: 0; display: flex; align-items: center; justify-content: center;">
          <div style="width: 90%; margin: 0; text-align: center;">
            <img src="static/figures/svg/5.1_actor.svg" width="100%" />
          </div>
        </div>
        <p class="caption" style="width: 90%; margin-left: 4%;">
          1, <b>Dots and lines depict the diffusion generation path from the policy</b>: BDPO accurately fits the target distribution (the rightmost column). <br />
          2, <b>Background color depicts the landscape of diffusion value function over the entire space</b>: In the initial steps ($n=50$ to $n=40$), the sample movement is subtle, while in the later steps ($n=30$ to $n=0$), the samples rapidly converge to the nearest modes of the data.
        </p>
      </div>

      <hr style="width: 50%; margin: 40px auto; border: none; border-top: 1px solid #ddd;" />

      <h3 style="text-align: center; color: #27579a; margin-bottom: 0px" id="d4rl">D4RL Benchmark Results
      </h3>
      <div class="yspace centered">
        <div class="full-width margin"
          style="width: 100%; margin: 0; display: flex; align-items: center; justify-content: center;">
          <div style="width: 90%; margin: 0; text-align: center;">
            <img src="static/figures/d4rl.png" width="100%" />
          </div>
        </div>
        <p class="caption" style="width: 90%; margin-left: 4%;">
          Diffusion-based methods, particularly those with diffusion-based actor and regularization (including BDPO, DAC, and Diffusion-QL), substantially outperform their non-diffusion counterparts, especially in locomotion tasks. 
          Meanwhile, BDPO consistently achieves superior performance across nearly all datasets, underscoring the effectiveness of combining diffusion policies and the behavior-regularized RL framework.
        </p>
      </div>

      <hr style="width: 50%; margin: 40px auto; border: none; border-top: 1px solid #ddd;" />

      <h3 style="text-align: center; color: #27579a; margin-bottom: 0px" id="runtime">Runtime Analysis
      </h3>
      <div class="yspace centered">
        <div class="full-width margin"
          style="width: 100%; margin: 0; display: flex; align-items: center; justify-content: center; gap: 20px;">
          <div style="width: 45%; margin: 0; text-align: center;">
            <img src="static/figures/svg/appfig_runtime.svg" width="100%" />
          </div>
          <p class="caption" style="width: 45%; margin: 0;">
            <b>BDPO runs efficiently.</b><br />
            BDPO consists of three key steps: pretraining behavior diffusion, training value functions ($Q^\pi$ and $V^{\pi,s}_n$), and optimizing the actor. 
            The pertaining phase takes around 8 minutes and is therefore negligible. 
            The $Q^\pi$ training follows the same approach as Diffusion-QL and DAC, while $V^{\pi,s}_n$ introduces acceptable overhead since it only requires single-step diffusion. 
            For actor training, both DAC and BDPO use single-step diffusion, while Diffusion-QL needs to back-propagate through the entire diffusion path, leading to a significantly higher runtime.
          </p>
        </div>
      </div>
    </section>

    <section>
      <h2>üèÅ Closing Remarks</h2>
      <div class="block full-width" style="margin-top: 0;">
        <div style="display: flex; align-items: center; gap: 20px;">
          <div style="flex: 1; margin: 0;">
            <p style="margin: 0;">
              <ul class="key-findings">
                <li>
                  Framing the reverse process of diffusion models as an MDP, we propose to implement the KL divergence w.r.t. the diffusion generation path, rather than the clean action samples;
                </li>
                <li>
                  Building upon this foundation, we propose a two-time-scale actor-critic method to optimize diffusion policies.  Instead of differentiating the policy along the entire diffusion path, BDPO estimates the values at intermediate diffusion steps to amortize the optimization, offering efficient computation, convergence guarantee and state-of-the-art performance;
                </li>
                <li>
                  Experiments conducted on synthetic 2D datasets reveal that our method effectively approximates the target distribution. Furthermore, when applied to continuous control tasks provided by D4RL, BDPO demonstrates superior performance compared to baseline offline RL algorithms.
                </li>
              </ul>
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- <section>
      <h2>üí° One More Thing: Flow RL</h2>
          <p>
            Running diffusion policies with PyTorch is really, really slow...
          </p>
    </section> -->

    <section>
      <h2>
        ${\bf B\kern-.05em{\small I\kern-.025em B}\kern-.08em
        T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}$
      </h2>
      <pre class="language-bibtex bibtex" id="bibtex" onclick="selectContent('#bibtex')"><code>@article{gao2025behavior,
    title={Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning},
    author={Gao, Chen-Xiao and Wu, Chenyang and Cao, Mingjun and Xiao, Chenjun and Yu, Yang and Zhang, Zongzhang},
    journal={arXiv preprint arXiv:2502.04778},
    year={2025}
}</code></pre>
    </section>
  <section>
    <div style="text-align: center; margin: 20px 0;">
      <p>This website template is based on <a href="https://github.com/wang-kevin3290/scaling-crl.github.io">scaling-crl.github.io</a> by Kevin Wang.</p>
    </div>
  </section>
  </main>
</body>

</html>