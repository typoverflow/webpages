<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description"
    content="Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <link rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.1.0/prism-bibtex.min.js"></script>
  <link rel="stylesheet" href="static/css/index.css" />

  <title>Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning</title>

  <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  <script>
    function selectContent(query) {
      var range = document.createRange()
      var selection = window.getSelection()
      var elem = document.querySelector(query)
      range.selectNodeContents(elem)
      selection.removeAllRanges()
      selection.addRange(range)
    }
  </script>
</head>

<body>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
        displayMath: [
          ["$$", "$$"],
          ["\\[", "\\]"],
        ],
        processEscapes: true,
        macros: {
          planclass: "\\operatorname{\\mathbf{plan}}",
          S: "\\mathcal{S}",
          gS: "\\mathcal{S}",
          cS: "\\mathcal{S}",
          cB: "\\mathcal{B}",
          cA: "\\mathcal{A}",
          A: "\\mathcal{A}",
          gA: "\\mathcal{A}",
          D: "\\mathcal{D}",
          R: "\\mathbb{R}",
          E: "\\mathbb{E}",
          P: "\\mathrm{P}",
          var: "\\mathrm{Var}",
          cov: "\\mathrm{Cov}",
          argmin: "\\mathop{\\arg\\min}",
          argmax: "\\mathop{\\arg\\max}",
        },
      },
      svg: {
        fontCache: "global",
      },
    }
  </script>
  <header>
    <h1>Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning</h1>
    <div class="authors">
      <span class="author">
        <a href="https://about.gaocx.io">Chen-Xiao Gao</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Chenyang Wu</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Mingjun Cao</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Chenjun Xiao</a></span>
      <span class="affil">2</span>
      <span class="author">
        <a>Yang Yu</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Zongzhang Zhang</a></span>
      <span class="affil">1</span>
    </div>
    <div class="notes">
      <span class="affil">1</span>
      <span class="institution">Nanjing University</span>
      <span class="affil">2</span>
      <span class="institution">The Chinese University of Hong Kong, Shenzhen</span>
    </div>
    <div class="links">
      <span class="link">
        <a href="./static/pdf/bdpo.pdf" target="_blank" class="button">
          <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
        </a>
      </span>
      <span class="link">
        <a href="https://arxiv.org/abs/2502.04778" class="button">
          <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
        </a>
      </span>
      <span class="link">
        <a href="https://github.com/typoverflow/flow-rl" class="button">
          <span class="icon"><i class="fas fa-code"></i></span><span>Code</span>
        </a>
      </span>
    </div>
  </header>

  <main>
    <section>
      <div class="summary">
        <h3 style="text-align: center; color: #27579a; margin-bottom: 0px">TL;DR</h3>
        <div class="yspace centered">
          <div class="full-width margin" 
               style="width: 100%; margin: 0; display: flex; align-items: center; justify-content: center; gap: 20px;">
            <div style="width: 45%; margin: 0; text-align: center;">
              <img src="static/figures/svg/path.svg" width="100%" />
            </div>
            <p class="caption" style="width: 45%; margin: 0;">
              Unimodal policies, such as deterministic policies, compute the behavior as the center of mass and therefore lead to misleading regularization; while we harnesses the flexibility of diffusion models, and the regularization is calculated as the accumulated discrepancies in diffusion directions of the actor and the behavior diffusion. This motivates BDPO, our efficient algorithm for optimizing diffusion policies. 
            </p>
          </div>
        </div>
      </div>
    </section>

    <section>
      <div class="abstract">
        <h3>Abstract</h3>
        <p>
          Behavior regularization, which constrains the policy to stay close to some behavior policy, is widely used in offline reinforcement learning to manage the risk of hazardous exploitation of unseen actions.
          Nevertheless, existing literature on behavior-regularized RL primarily focuses on explicit policy parameterizations, such as Gaussian policies. 
          Consequently, it remains unclear how to extend this framework to more advanced policy parameterizations, such as diffusion models. 
          In this paper, we introduce BDPO, a principled behavior-regularized RL framework tailored for diffusion-based policies, thereby combining the expressive power of diffusion policies and the robustness provided by regularization. 
          The key ingredient of our method is to calculate the Kullback-Leibler (KL) regularization analytically as the accumulated discrepancies in reverse-time transition kernels along the diffusion trajectory. 
          By integrating the regularization, we develop an efficient two-time-scale actor-critic RL algorithm that produces the optimal policy while respecting the behavior constraint. 
          Comprehensive evaluations conducted on synthetic 2D tasks and continuous control tasks from the D4RL benchmark validate its effectiveness and superior performance. 
        </p>
      </div>
    </section>

    <section>
      <h2>Difficulties of Combining Diffusion Policies and Offline RL</h2>
          <p>
          The popularized behavior-regularized RL framework designs to optimize policies $\pi$ while also constraining them towards a behavior policy $\nu$: 
          $$\max_\pi\ \mathbb{E}_{a\sim\pi(\cdot|s)}\left[Q(s, a)\right]-\eta D_\mathrm{KL}(\pi\|\nu).$$
          By specifying $\nu$ as ...
          <ul style="list-style-type: disc; margin-left: 20px;">
            <li>
              <b>the uniform policy $U[\mathcal{A}]$:</b> it encourages diversity and exploration of the policy, recovering Maximum Entropy RL; 
            </li>
            <li>
              <b>the dataset collection policy $\pi_{\mathcal{D}}$:</b> it restricts the policy to center around the dataset $\mathcal{D}$, balancing optimization and safety;
            </li>
            <li>
              <b>the pre-trained or sft policy $\pi_{\mathrm{ref}}:</b> it prevent the mode collapse during post-training. 
            </li>
          </ul>
          However, combining diffusion policies with the behavior-regularized RL framework is non-trivial.  
          </p>
          <div class="columns bypad centered" style="width: 100%; margin: 0; display: flex;">
            <div style="color: #6e0909; width: 48%; margin-right: 2%;">
              <p class="caption">
                <b>❌ Problem 1: Regularization Calculation</b><br />
                Traditional regularization, such as KL divergence, requires action densities, which is not directly available for diffusion policies. 
              </p>
            </div>
            <div style="color: #6e0909; width: 48%; margin-left: 2%; margin-top: -0px;">
              <p class="caption">
                <b>❌ Problem 2: Diffusion Policy Optimization</b><br />
                Traditional training objectives of diffusion models require samples from the target distribution, while in offline RL, we only have access to samples from the behavior policy $\nu$.
              </p>
            </div>
          </div>
    </section>

    <section>
      <h2>The Ingredients of BDPO</h2>
      <div class="building-blocks-grid">

        <div class="block full-width" style="margin-top: 0;">
          <div style="display: flex; align-items: center; gap: 20px;">
            <div style="flex: 1; margin: 0;">
              <h3 style="margin-top: 0;">1. Pathwise KL Regularization</h3>
              <p style="margin: 0;">
                Instead of the considering the KL divergence between action distributions, we consider the KL divergence defined on diffusion paths, which can be further decomposed into the accumulated discrepancies in reverse diffusion directions:
                $$
                \begin{aligned}
                &D_{\rm KL}(p^{\pi, s}_{0:N} \| p^{\nu, s}_{0:N})\\
                &=\mathbb{E}\left[\log \frac{p_N^{\pi,s}(a^N)\prod_{n=1}^Np_{n-1|n}^{\pi,s}(a^{n-1}|a^n)}{p_N^{\nu,s}(a^N)\prod_{n=1}^Np_{n-1|n}^{\nu,s}(a^{n-1}|a^n)}\right]\\
                &=\mathbb{E}\left[\sum_{n=1}^{N}D_{\rm KL}(p^{\pi, s,a^n}_{n-1|n}\|p^{\nu, s,a^n}_{n-1|n})\right]\\
                &=\mathbb{E}\left[\sum_{n=1}^{N}\frac{\|\mu^{\pi,s}_n(a^n)-\mu^{\nu,s}_n(a^n)\|^2}{2\sigma_n^2}\right].
                \end{aligned}
                $$
                We therefore consider using the pathwise KL regularized objective:
                $$
                \max_{p^\pi}\ \mathbb{E}_{a^0\sim p^{\pi,s}}\left[Q(s, a^0)\right]-\eta D_{\rm KL}(p^{\pi, s}_{0:N} \| p^{\nu, s}_{0:N}).
                $$
              </p>
              <p style="color: #27579a">
                ✔ We proved the equivalency between pathwise KL regularization and KL regularization. <br />
                ✔ The pathwise KL is consistent with the KL between two diffusion SDEs in the limit of infinitesimal step sizes.
              </p>
            </div>
          </div>
        </div>

        <div class="block full-width" style="margin-top: 0;">
          <div style="display: flex; align-items: center; gap: 20px;">
            <div style="flex: 1; margin: 0;">
              <h3 style="margin-top: 0;">2. Two-time-scale Actor-Critic</h3>
              <p style="margin: 0;">
                Instead of the considering the KL divergence between action distributions, we consider the KL divergence defined on diffusion paths, which can be further decomposed into the accumulated discrepancies in reverse diffusion directions:
                $$
                \begin{aligned}
                &D_{\rm KL}(p^{\pi, s}_{0:N} \| p^{\nu, s}_{0:N})\\
                &=\mathbb{E}\left[\log \frac{p_N^{\pi,s}(a^N)\prod_{n=1}^Np_{n-1|n}^{\pi,s}(a^{n-1}|a^n)}{p_N^{\nu,s}(a^N)\prod_{n=1}^Np_{n-1|n}^{\nu,s}(a^{n-1}|a^n)}\right]\\
                &=\mathbb{E}\left[\sum_{n=1}^{N}D_{\rm KL}(p^{\pi, s,a^n}_{n-1|n}\|p^{\nu, s,a^n}_{n-1|n})\right]\\
                &=\mathbb{E}\left[\sum_{n=1}^{N}\frac{\|\mu^{\pi,s}_n(a^n)-\mu^{\nu,s}_n(a^n)\|^2}{2\sigma_n^2}\right].
                \end{aligned}
                $$
                We therefore consider using the pathwise KL regularized objective:
                $$
                \max_{p^\pi}\ \mathbb{E}_{a^0\sim p^{\pi,s}}\left[Q(s, a^0)\right]-\eta D_{\rm KL}(p^{\pi, s}_{0:N} \| p^{\nu, s}_{0:N}).
                $$
              </p>
              <p style="color: #27579a">
                ✔ We proved the equivalency between pathwise KL regularization and KL regularization. <br />
                ✔ The pathwise KL is consistent with the KL between two diffusion SDEs in the limit of infinitesimal step sizes.
              </p>
            </div>
          </div>
        </div>
      </div>

      <style>
        .building-blocks-grid {
          display: grid;
          grid-template-columns: repeat(2, 1fr);
          gap: 20px;
          margin: 20px 0;
          perspective: 1000px;
        }

        .block {
          background-color: #f5f5f5;
          border: 1px solid #ddd;
          border-radius: 8px;
          padding: 20px;
          transition: all 0.3s ease;
          position: relative;
          overflow: hidden;
          transform-style: preserve-3d;
        }

        .block h3 {
          color: #27579a;
          margin-top: 0;
        }

        .block p {
          margin-bottom: 0;
        }

        .full-width {
          grid-column: 1 / -1;
          margin-top: 20px;
        }

        /* Add hover effects */
        .block:hover {
          transform: translateY(-5px);
          box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
        }

        h3#qualitative-evaluation {
          scroll-margin-top: 20px;
          text-align: center;
          color: #27579a;
        }
      </style>

      <script>
        document.addEventListener('DOMContentLoaded', () => {
          const blocks = document.querySelectorAll('.block');

          blocks.forEach(block => {
            block.addEventListener('mousemove', (e) => {
              const rect = block.getBoundingClientRect();
              const x = e.clientX - rect.left;
              const y = e.clientY - rect.top;

              const centerX = rect.width / 2;
              const centerY = rect.height / 2;

              // Special case for full-width block - reduce rotation by half
              const divisor = block.classList.contains('full-width') ? 1500 : 320;
              const rotateX = -(y - centerY) / divisor;
              const rotateY = (x - centerX) / divisor;

              block.style.transform = `
                                perspective(1000px)
                                rotateX(${rotateX}deg)
                                rotateY(${rotateY}deg)
                                scale3d(1.005, 1.005, 1.005)
                            `;

              block.style.boxShadow = `
                                ${-rotateY / 16}px ${rotateX / 16}px 20px rgba(39, 87, 207, 0.15),
                                ${-rotateY / 32}px ${rotateX / 32}px 10px rgba(39, 87, 207, 0.11)
                            `;
            });

            block.addEventListener('mouseleave', () => {
              block.style.transform = 'perspective(1000px) rotateX(0) rotateY(0) scale3d(1, 1, 1)';
              block.style.boxShadow = 'none';
            });
          });
        });
      </script>
    </section>

    <section>
      <h2>Summary of Key Empirical Findings</h2>
      <div class="findings-container">
        <ul class="key-findings">
          <li>
            CRL is scalable to depths unattainable by other RL proprioceptive algorithms (1000+ layers),
            perhaps due
            to
            its self-supervised nature.
          </li>
          <li>
            Both width and depth are key factors influencing CRL's performance, but depth achieves greater
            performance
            and better parameter-efficiency (similar performance for 50× smaller models).
          </li>
          <li>
            We observe signs of emergent behaviors in CRL with deep neural networks, such as humanoid
            learning to walk
            and navigate a maze.
          </li>
          <li>
            Scale unlocks learning difficult maze topologies.
          </li>
          <li>
            Batch size scaling occurs in CRL for deep networks.
          </li>
          <li>
            CRL benefits from both the actor and critic scale.
          </li>
        </ul>
      </div>

      <style>
        .findings-container {
          background-color: #f5f5f5;
          border: 1px solid #ddd;
          border-radius: 8px;
          padding: 25px 30px;  /* Increased padding */
          margin: 20px 0;
          width: 100%;        /* Changed from 70% to 100% */
          box-sizing: border-box;
        }

        .key-findings {
          margin: 0;
          padding-left: 25px;  /* Slightly increased padding */
          columns: 2;         /* Split into 2 columns */
          column-gap: 40px;   /* Space between columns */
        }

        .key-findings li {
          margin-bottom: 15px;  /* Increased spacing between items */
          break-inside: avoid;  /* Prevent items from splitting across columns */
        }

        .key-findings li:last-child {
          margin-bottom: 0;
        }
      </style>
    </section>

    <section>
      <h2>
        ${\bf B\kern-.05em{\small I\kern-.025em B}\kern-.08em
        T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}$
      </h2>
      <!--  <pre class="language-bibtex bibtex" id="bibtex" onclick="selectContent('#bibtex')"><code>@inproceedings{myers2025horizon,
    author    = {Myers, Vivek and Ji, Catherine and Eysenbach, Benjamin},
    booktitle = {{International Conference} on {Learning Representations}},
    title     = {{Horizon Generalization} in {Reinforcement Learning}},
    url       = {https://arxiv.org/abs/2501.02709},
    year      = {2025}, -->
      <pre class="language-bibtex bibtex" id="bibtex" onclick="selectContent('#bibtex')"><code>@article{gao2025behavior,
    title={Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning},
    author={Gao, Chen-Xiao and Wu, Chenyang and Cao, Mingjun and Xiao, Chenjun and Yu, Yang and Zhang, Zongzhang},
    journal={arXiv preprint arXiv:2502.04778},
    year={2025}
}</code></pre>
    </section>
  <section>
    <div style="text-align: center; margin: 20px 0;">
      <p>This website template is based on <a href="https://github.com/wang-kevin3290/scaling-crl.github.io">scaling-crl.github.io</a> by Kevin Wang.</p>
    </div>
  </section>
  </main>
</body>

</html>